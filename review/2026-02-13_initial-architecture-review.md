# Review Report: AI Stuff Web - Initial Architecture

**Date:** 2026-02-13
**Phase:** Project Initialization & VLM Module
**Status:** Implementation Complete, Pending Deployment

---

## 1. Client Requirements

The client requested:
1. An **AI portfolio website** hosted on GitHub Pages
2. A landing page with a **game/arcade-style UI** showcasing AI projects
3. First project: **VLM (Vision Language Model)** with live camera interaction
4. **No local computer required** — everything must work from GitHub web interface only

## 2. Feasibility Analysis

### Initial Approach: Ollama (REJECTED)

The first consideration was to use Ollama for VLM inference.

**Why it was rejected:**
- Ollama requires a local server process (`localhost:11434`)
- The client explicitly stated they have no local computer
- GitHub Pages cannot host backend services
- Even if the frontend worked, there would be no Ollama instance to connect to

### Approved Approach: Browser-Side AI via Transformers.js

**Why this works:**
- Transformers.js runs ML models entirely within the browser using WebAssembly/WebGPU
- Zero server dependency — the model weights are downloaded and cached by the browser
- GitHub Pages only needs to serve static HTML/JS/CSS files
- Camera access uses standard WebRTC APIs (browser-native)

**Trade-offs acknowledged:**
| Aspect | Impact |
|--------|--------|
| Model Size | Limited to smaller models (~50-300MB) that browsers can handle |
| Model Capability | Less capable than full Ollama models (e.g., LLaVA 7B) |
| First Load | Users must download model weights on first visit |
| Performance | Depends on visitor's device; mobile will be slower |
| Prompt Flexibility | Image-to-text pipeline is simpler than full VLM chat |

## 3. Architecture Decisions

### Tech Stack

| Component | Choice | Rationale |
|-----------|--------|-----------|
| Build Tool | Vite | Fast, modern, minimal config, tree-shaking |
| Framework | Vanilla JS | No framework overhead for a portfolio site |
| AI Runtime | Transformers.js (HuggingFace) | Only viable option for browser-based ML |
| Model | `Xenova/vit-gpt2-image-captioning` | Small (~100MB), fast, reliable image captioning |
| Deployment | GitHub Pages + Actions | Auto-deploy on push, free hosting |
| Styling | Custom CSS | Arcade/retro theme, no CSS framework needed |

### Project Structure

```
AI_stuff_web/
├── index.html              # Landing page (arcade grid)
├── vite.config.js          # Build configuration
├── package.json            # Dependencies
├── .github/workflows/
│   └── deploy.yml          # GitHub Pages auto-deploy
├── src/
│   ├── pages/
│   │   └── vlm.html        # VLM camera page
│   ├── styles/
│   │   ├── main.css         # Global styles (arcade theme)
│   │   └── vlm.css          # VLM-specific styles
│   └── vlm-app.js          # VLM application logic
├── review/                 # Architecture review reports
└── skills/                 # Claude Code skill files
```

### Design Language

- **Theme:** Dark, CRT/arcade aesthetic with scanline overlay
- **Color Palette:** Cyan accent (#00e5ff) on dark background (#0a0a0f)
- **Typography:** Monospace (Courier New / Consolas)
- **Animations:** Scanline sweep, pulse glow, blink indicators

## 4. VLM Module Design

### Data Flow
```
Camera (WebRTC) → Video Element → Canvas Capture → Base64 JPEG
    → Transformers.js Pipeline → Generated Text → Result Display
```

### Features Implemented
- Camera enable/disable with environment-facing preference
- Single-frame capture and analysis
- Auto-mode: analyze every 5 seconds
- Prompt presets (Describe, Objects, Read Text, Mood)
- Model loading progress bar with percentage
- Connection status indicator
- Captured frame preview
- Error handling for camera/model failures

### Limitations (Current Version)
- The `vit-gpt2-image-captioning` model generates simple captions, not full conversational VLM responses
- Custom prompts are displayed but the current model doesn't support prompt-conditioned generation
- Future upgrade path: switch to a multimodal model when browser-compatible options mature

## 5. Deployment Strategy

- GitHub Actions workflow triggers on push to `main`, `master`, or the feature branch
- Vite builds static assets to `dist/`
- `actions/deploy-pages@v4` publishes to GitHub Pages
- Base URL set to `/AI_stuff_web/` to match repository name

## 6. Next Steps & Recommendations

1. **Merge to main** to activate GitHub Pages deployment
2. **Enable GitHub Pages** in repo settings (Source: GitHub Actions)
3. **Test on mobile** — camera and model loading should work on modern phones
4. **Consider model upgrade** — Florence-2 or MoonDream when Transformers.js support matures
5. **Add more AI projects** — the arcade grid is designed to accommodate new cards

---

*Report generated by Claude Code — AI Portfolio Project*
